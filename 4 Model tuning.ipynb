{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this phase is to find best parameters for chosen algorithms: RandomForestRegressor and XGBoostRegressor. So far I worked on a sample of data. I will now use whole dataset to tune parametes therefore I would need to preprocess the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "mport xgboost as xgb \n",
    "from sklearn.model_selection import validation_curve, GridSearchCV, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set column types to minimize DataFrame\n",
    "column_types= {'agencyId': 'int16',\n",
    " 'clouds_all': 'int8',\n",
    " 'delayInSeconds': 'int64',\n",
    " 'humidity': 'int8',\n",
    " 'id': 'int32',\n",
    " 'pressure': 'int16',\n",
    " 'routeId': 'int16',\n",
    " 'stopId': 'int32',\n",
    " 'stopLat': 'float64',\n",
    " 'stopLon': 'float64',\n",
    " 'stopSequence': 'int32',\n",
    " 'temp': 'float64',\n",
    " 'tripId': 'int16',\n",
    " 'vehicleId': 'int32',\n",
    " 'wind_deg': 'int16',\n",
    " 'wind_speed': 'int8'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create DataFrame \n",
    "df = pd.read_csv('train.csv', dtype=column_types, parse_dates=['delayPredictionTimestamp','scheduleTime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now peprocess whole dataset using code written in preprocessing phase. I will additionaly remove/not merge columns that I decided in model creation phase that are not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge stopsintrips file.Merge stopSequence on:'routeId','stopId', 'tripId', merge agency ID on route ID only to have minimal number of missing rows\n",
    "stops=pd.read_csv('stopsintrips.csv', dtype=column_types)\n",
    "stops_1=stops[['routeId','agencyId']].drop_duplicates()\n",
    "df=df.merge(right=stops_1, how='left', on=['routeId'])\n",
    "df=df.merge(right=stops[['routeId', 'stopId', 'stopSequence', 'tripId']], how='left', on=['routeId','stopId', 'tripId'])\n",
    "# merge stops file containig location. Not all locations exist in stops file.\n",
    "location=pd.read_csv('stops.csv', dtype=column_types)\n",
    "missing_location=df[~df['stopId'].isin(location['stopId'])]['stopId'].unique().reshape(-1,1)\n",
    "# as there is linear correlation between location and stopID, missing data to be predicted by LinearRegression model\n",
    "# predict missing stopLat\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X=location.iloc[:,:1].values\n",
    "y=location['stopLat'].values\n",
    "lr_lat=LinearRegression()\n",
    "lr_lat.fit(X,y)\n",
    "# predict missing stopLon\n",
    "X=location.iloc[:,:1].values\n",
    "y=location['stopLon'].values\n",
    "lr_lon=LinearRegression()\n",
    "lr_lon.fit(X,y)\n",
    "# add predicted missing locations to location df\n",
    "missing_lon=lr_lon.predict(missing_location)\n",
    "missing_lat=lr_lat.predict(missing_location)\n",
    "missing_loc_df=pd.DataFrame({'stopId':np.squeeze(missing_location), 'stopLat':missing_lat, 'stopLon':missing_lon})\n",
    "location=pd.concat([location,missing_loc_df],axis=0)\n",
    "# merge locations with df\n",
    "df=df.merge(right=location, how='left', left_on=['stopId'], right_on=['stopId'])\n",
    "# merge weather df. Remove duplicates. Forward fill missing hour rows. Drop columns that do not add value (tested)\n",
    "weather=pd.read_csv('weatherHistory.csv',sep=';')\n",
    "weather['dt']=pd.to_datetime(weather['dt'],unit='s')\n",
    "weather.drop_duplicates(subset='dt', keep='first',inplace=True)\n",
    "weather=weather.set_index(pd.DatetimeIndex(weather['dt']))\n",
    "weather=weather.resample('H').ffill()\n",
    "weather.drop(['dt','dt_iso', 'city_id', 'temp_min', 'temp_max','weather_icon','weather_description','weather_id', 'weather_main', 'pressure', 'clouds_all'], axis=1, inplace=True)\n",
    "cols=['humidity','wind_speed','wind_deg']\n",
    "for col in cols: weather[col]=pd.to_numeric(weather[col], downcast='signed')\n",
    "df['dt']=df['scheduleTime'].dt.round('h')\n",
    "df=df.merge(right=weather, how='left', left_on='dt', right_index=True)\n",
    "df.drop(['dt'], axis=1, inplace=True)\n",
    "# deal with other missing data. AgencyID: fill with most common value '1', stopSequence: fill with median '12'\n",
    "df['agencyId']=df['agencyId'].fillna(1)\n",
    "df['stopSequence']=df['stopSequence'].fillna(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will add additional features tested in model creaiton phase and create function to exclude 'id' and 'delayInSeconds' from features matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# difference between delayPrediction and schedule time. Wrong date when time close to midnight need to add or subtract 24h\n",
    "df['time_diff']=(df['delayPredictionTimestamp']-df['scheduleTime']).astype('timedelta64[s]')\n",
    "df['time_diff']=pd.to_numeric(df['time_diff'], downcast='signed')\n",
    "df['time_diff']=df['time_diff'].apply(lambda x: x+86400 if x <-50000 else x)\n",
    "df['time_diff']=df['time_diff'].apply(lambda x: x-86400 if x >50000 else x)\n",
    "# time features: hour, dayofweek, time- day time in seconds\n",
    "df['hour'] = df['scheduleTime'].dt.hour\n",
    "df['dayofweek'] = df['scheduleTime'].dt.dayofweek\n",
    "df['time']=df['delayPredictionTimestamp'].dt.second+df['delayPredictionTimestamp'].dt.minute*60+df['delayPredictionTimestamp'].dt.hour*3600\n",
    "# mean delayInSeconds of each agency, routeId, stopID, vehicleId, tripID\n",
    "agencyMeanDelay = df[ ['agencyId', 'delayInSeconds'] ].groupby(['agencyId']).mean().to_dict()['delayInSeconds']\n",
    "df['agencyMeanDelay'] = df['agencyId'].map(lambda x: agencyMeanDelay[x])\n",
    "routeMeanDelay = df[ ['routeId', 'delayInSeconds'] ].groupby(['routeId']).mean().to_dict()['delayInSeconds']\n",
    "df['routeMeanDelay'] = df['routeId'].map(lambda x: routeMeanDelay[x])\n",
    "stopIdMeanDelay = df[ ['stopId', 'delayInSeconds'] ].groupby(['stopId']).mean().to_dict()['delayInSeconds']\n",
    "df['stopIdMeanDelay'] = df['stopId'].map(lambda x: stopIdMeanDelay[x])\n",
    "vehicleIdMeanDelay = df[ ['vehicleId', 'delayInSeconds'] ].groupby(['vehicleId']).mean().to_dict()['delayInSeconds']\n",
    "df['vehicleIdMeanDelay'] = df['vehicleId'].map(lambda x: vehicleIdMeanDelay[x])\n",
    "tripIdMeanDelay = df[ ['tripId', 'delayInSeconds'] ].groupby(['tripId']).mean().to_dict()['delayInSeconds']\n",
    "df['tripIdMeanDelay'] = df['tripId'].map(lambda x: tripIdMeanDelay[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define features function to exclude some columns from features matrix, remove features with very low importancies\n",
    "def features(df):\n",
    "    feats = df.columns.values\n",
    "    black_list = ['id', 'delayInSeconds']\n",
    "    return [feat for feat in feats if feat not in black_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to create features matrix and dependant variable array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=feats_engineering(df)[features].values\n",
    "y=df['delayInSeconds'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will first check what trees depth gives the best model accuracy. I will use validation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_depth = [6,8,10,12,14,16,18]\n",
    "train_score, val_score = validation_curve(RandomForestRegressor(n_estimators=10,random_state=2018,n_jobs=-1),\n",
    "                                          X, y, 'max_depth', max_depth,  cv=3, scoring= 'neg_mean_squared_error')\n",
    "plt.plot(max_depth, np.median(np.sqrt(-train_score),axis= 1), color='blue', label='training score')\n",
    "plt.plot(max_depth, np.median(np.sqrt(-val_score),axis=1), color='red', label='validation score')\n",
    "plt.legend()\n",
    "plt.ylim(50, 200)\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('root mean squared error');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I will use grid search to look for best combination of max_depth, min_samples_leaf and n_estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf=RandomForestRegressor()\n",
    "param_grid = {'max_depth': [12,14],\n",
    "              'min_samples_leaf': [3,5,10],\n",
    "              'n_estimators': [20,30]}\n",
    "grid=GridSearchCV(rf, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid.fit(X,y)\n",
    "print(grid.grid_scores_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I won't present thr whole process of grid search but I checked more combiations and finally chose the following parameters:\n",
    "max_depth=12, min_samples_leaf=5, n_estimators=50. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I tuned parameters of XGBoost in the same way and chose following parameters: max_depth=3, learning_rate=0.1, n_estimators=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_models():\n",
    "     return [\n",
    "        ('rf', RandomForestRegressor(max_depth=12,n_estimators=50,, min_samples_leaf=5, random_state=2018, n_jobs=-1)),\n",
    "        ('xgbr', xgb.XGBRegressor( max_depth=3, learning_rate=0.1, n_estimators=300, n_jobs=-1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compare results\n",
    "for name, model in create_models():\n",
    "    scores=cross_validate(model, X, y,cv=5, scoring='neg_mean_squared_error')\n",
    "    print ( name + ' train_score:' + str(np.round(np.sqrt(-scores['train_score'].mean()),3))+ ' test_score:' + str(np.round(np.sqrt(-scores['test_score'].mean()),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the best result ... was achieved by XGBoost. It's a significant improvement in comparisont to the result obtained using DummyRegressor: .... My final idea is to combine predictions made by XGBmodel with RandomForest ones, as usually combination of predictors create a better predictor. I will check it in prediction phase on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
